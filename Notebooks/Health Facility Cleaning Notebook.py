#!/usr/bin/env python
# coding: utf-8

# ## Health Facility Cleaning Notebook
# 
# New notebook

# In[ ]:


# install pandas
get_ipython().system('pip install pandas --upgrade google-api-python-client google-auth google-auth-oauthlib')


# In[ ]:


import os
import io
import pandas as pd
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload

# ‚úÖ Microsoft Fabric Lakehouse file path
SERVICE_ACCOUNT_FILE = "/lakehouse/default/Files/service_account.json"
# ‚úÖ Google Drive read-only scope
SCOPES = ["https://www.googleapis.com/auth/drive.readonly"]

# ‚úÖ Authenticate with Google
creds = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)

# ‚úÖ Folder ID in Google Drive
FOLDER_ID = "1lXiB1txJNb0kZM-Vk5TtIgFOasHgKeja"

def list_files_in_folder(service, folder_id):
    """ List all files inside a Google Drive folder """
    query = f"'{folder_id}' in parents and mimeType='text/csv'"
    files = []
    page_token = None
    while True:
        results = service.files().list(
            q=query,
            fields="nextPageToken, files(id, name, mimeType)",
            pageToken=page_token
        ).execute()
        files.extend(results.get("files", []))
        page_token = results.get("nextPageToken")
        if not page_token:
            break
    return files

def download_file_to_memory(service, file_id):
    """ Download file as BytesIO (in-memory) """
    request = service.files().get_media(fileId=file_id)
    fh = io.BytesIO()
    downloader = MediaIoBaseDownload(fh, request)
    done = False
    while not done:
        _, done = downloader.next_chunk()
    fh.seek(0)
    return fh
def clean_column_names(df):
    """Sanitize column names to be Delta Lake compatible"""
    cleaned_columns = [col.strip().replace(" ", "_").replace("(", "").replace(")", "").replace(",", "")
                       .replace(";", "").replace("{", "").replace("}", "").replace("\n", "")
                       .replace("\t", "").replace("=", "") for col in df.columns]
    df.columns = cleaned_columns
    return df

def create_table_from_csv(file_stream, table_name):
    """Load CSV, clean column names, write to Lakehouse as Spark table"""
    try:
        df_pd = pd.read_csv(file_stream)

        # Clean column names for Delta Lake
        df_pd = clean_column_names(df_pd)

        # Convert to Spark DataFrame
        df_spark = spark.createDataFrame(df_pd)

        # Write to Lakehouse table
        df_spark.write.mode("overwrite").saveAsTable(table_name)
        print(f"‚úÖ Table '{table_name}' created successfully.")
    except Exception as e:
        print(f"‚ùå Failed to create table: {e}")


def download_and_ingest_files():
    try:
        service = build("drive", "v3", credentials=creds)
        files = list_files_in_folder(service, FOLDER_ID)

        if not files:
            print("üìÇ No CSV files found in the folder.")
            return

        for file in files:
            print(f"üì• Processing file: {file['name']}")
            file_stream = download_file_to_memory(service, file["id"])
            table_name = file['name'].replace(".csv", "").replace(" ", "_").lower()
            create_table_from_csv(file_stream, table_name)

        print("\n‚úÖ All CSV files ingested into Lakehouse tables.")
    except Exception as e:
        print(f"‚ùå Error: {e}")

# üöÄ Run the process
download_and_ingest_files()


# In[ ]:





# In[ ]:


# Code generated by Data Wrangler for PySpark DataFrame

from pyspark.ml.feature import Imputer
from pyspark.sql import functions as F

def clean_data(df):
    # Replace missing values with the median of each column in: 'Reported_Cases'
    # ‚ö†Ô∏è The median in PySpark is approximated for performance reasons.
    cols = ['Reported_Cases']
    imputer = Imputer(inputCols=cols, outputCols=cols, strategy='median')
    df = imputer.fit(df).transform(df)
    # Replace all instances of "" with "Threat must include activity." in column: 'Notes'
    df = df.withColumn('Notes', F.regexp_replace('Notes', "(?i)^$", "Threat must include activity."))
    # Replace all instances of "Missing value" with "Threat must include activity." in column: 'Notes'
    df = df.withColumn('Notes', F.regexp_replace('Notes', "(?i)^Missing value$", "Threat must include activity."))
    # Replace missing values with "Threat must include activity." in column: 'Notes'
    df = df.fillna(value="Threat must include activity.", subset=['Notes'])
    # Replace missing values with the most common value of each column in: 'County', 'Date_Reported'
    cols = ['County', 'Date_Reported']
    imputer = Imputer(inputCols=cols, outputCols=cols, strategy='mode')
    df = imputer.fit(df).transform(df)
    return df

df_clean = clean_data(df)
display(df_clean)


# In[ ]:


df = spark.sql("SELECT * FROM LakehouseSilver.health_facility_reports_1 LIMIT 1000")
display(df)


# In[ ]:


from pyspark.ml.feature import Imputer
from pyspark.sql import functions as F
from pyspark.sql import types as T
import random
from datetime import datetime, timedelta

# UDF to generate random timestamp between 2020 and 2024
def get_random_date():
    start_date = datetime(2020, 1, 1)
    end_date = datetime(2024, 12, 31)
    delta = end_date - start_date
    random_days = random.randint(0, delta.days)
    return start_date + timedelta(days=random_days)

random_date_udf = F.udf(lambda: get_random_date(), T.TimestampType())

# UDF to randomly assign health clinic notes
notes_list = [
    "Patient referred to clinic",
    "Follow-up required",
    "Vaccination administered",
    "Discharged with medication",
    "Sample sent to lab",
    "High risk zone alert",
    "Contact tracing ongoing"
]
random_note_udf = F.udf(lambda: random.choice(notes_list), T.StringType())

# UDF to generate Reported_Deaths <= Reported_Cases
def generate_deaths(cases):
    if cases is None or cases <= 0:
        return 0
    if random.random() < 0.7:  # 70% chance of zero deaths
        return 0
    else:
        return random.randint(1, int(cases))  # at least 1 and ‚â§ cases

generate_deaths_udf = F.udf(generate_deaths, T.ByteType())

# Full cleaning function
def clean_data(df):
    # Cast columns
    df = df.withColumn('County', df['County'].cast(T.StringType()))
    df = df.withColumn('Reported_Cases', df['Reported_Cases'].cast(T.FloatType()))
    df = df.withColumn('Disease', df['Disease'].cast(T.StringType()))
    df = df.withColumn('Facility_Name', df['Facility_Name'].cast(T.StringType()))
    df = df.withColumn('Facility_ID', df['Facility_ID'].cast(T.StringType()))

    # Fill NULL in 'County' with mode
    mode_county_row = df.groupBy("County").count().orderBy(F.desc("count")).first()
    mode_county = mode_county_row["County"] if mode_county_row else None
    if mode_county:
        df = df.fillna({"County": mode_county})

    # Impute Reported_Cases using median
    imputer = Imputer(inputCols=['Reported_Cases'], outputCols=['Reported_Cases'], strategy='median')
    df = imputer.fit(df).transform(df)

    # Randomize Date_Reported and Notes
    df = df.withColumn('Date_Reported', random_date_udf())
    df = df.withColumn('Notes', random_note_udf())

    # Generate realistic Reported_Deaths
    df = df.withColumn('Reported_Deaths', generate_deaths_udf(df['Reported_Cases']))

    return df

# Apply transformation
df_clean = clean_data(df)

# Display result
display(df_clean)


# In[ ]:


# ‚úÖ Save to the default Lakehouse tables as 'Health_Facility_Reports_Cleaned_New'
df_clean.write.mode("overwrite").saveAsTable("Health_Facility_Reports_Cleaned_New")

print("‚úÖ Table 'Health_Facility_Reports_Cleaned_New' saved to default Lakehouse.")


# In[ ]:


df = spark.sql("SELECT * FROM LakehouseSilver.mobile_health_silver LIMIT 1000")
display(df)


# In[ ]:


df_clean.write.mode("overwrite").saveAsTable("your_table_name")

